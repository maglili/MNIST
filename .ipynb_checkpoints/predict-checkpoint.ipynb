{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predit pics\n",
    "## import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in CPU\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "composed = transforms.Compose([transforms.Grayscale(num_output_channels=1), \\\n",
    "                               transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), interpolation = 2), \\\n",
    "                               transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "    \n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('GPU is avalible')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Training in CPU')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "### loading model(invert)\n",
    "<a href=\"https://www.researchgate.net/figure/Architecture-of-a-Convolutional-Neural-Network-CNN-The-traditional-CNN-structure-is_fig1_330106889\"><img src=\"https://www.researchgate.net/publication/330106889/figure/fig1/AS:710963951063040@1546518423301/Architecture-of-a-Convolutional-Neural-Network-CNN-The-traditional-CNN-structure-is.png\" alt=\"Architecture of a Convolutional Neural Network (CNN). The traditional CNN structure is mainly composed of convolution layers, pooling layers, fully connected layers, and some activation functions. Each convolution kernel is connected to the part of feature maps. The input is connected to all of the output elements in the fully connected layer.\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, out_1=64, out_2=128):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(out_2 * 7 * 7, 10)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "model = CNN(out_1=64, out_2=128)\n",
    "model.load_state_dict(torch.load('invert_color.pt', map_location=device)) #<--------------------change model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1152x648 at 0x26BD7431D60>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = Image.open(r\"D:\\user\\Documents\\N26091194_TENG\\Projects\\MNIST\\pics\\22.jpg\")#<-----change your pics path\n",
    "print(image)\n",
    "X = composed(image)\n",
    "X = X[None, :, :]\n",
    "show_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推測 0 的機率: 10.9 %\n",
      "推測 1 的機率: 78.9 %\n",
      "推測 2 的機率: 3.23 %\n",
      "推測 3 的機率: 0.147 %\n",
      "推測 4 的機率: 0.118 %\n",
      "推測 5 的機率: 3.85 %\n",
      "推測 6 的機率: 0.76 %\n",
      "推測 7 的機率: 1.87 %\n",
      "推測 8 的機率: 0.0327 %\n",
      "推測 9 的機率: 0.208 %\n",
      "----------------------------------------\n",
      "預測結果 1st : 1\n",
      "預測結果 2nd : 0\n",
      "預測結果 3rd : 5\n",
      "----------------------------------------\n",
      "預測為 : 1\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "z = model(X)\n",
    "z = nn.functional.softmax(z, dim=1)\n",
    "output = z.tolist()[0]\n",
    "output_copy = output.copy()\n",
    "\n",
    "for index, value in enumerate(output):\n",
    "    print('推測 '+str(index)+' 的機率: {:.3} %'.format(value * 100))\n",
    "print('-'*40)\n",
    "\n",
    "print('預測結果 1st :', output.index(max(output_copy)))\n",
    "output_copy.remove(max(output_copy))\n",
    "\n",
    "print('預測結果 2nd :', output.index(max(output_copy)))\n",
    "output_copy.remove(max(output_copy))\n",
    "\n",
    "print('預測結果 3rd :', output.index(max(output_copy)))\n",
    "output_copy.remove(max(output_copy))\n",
    "\n",
    "print('-'*40)\n",
    "print('預測為 :',output.index(max(output)))\n",
    "\n",
    "# model: invert_color.pt\n",
    "# error in black : 1, 3, 6, 7, 9 (acc: 50%)\n",
    "# error in white : 0, 6, 7, 9 (acc: 60%)\n",
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "### loading model\n",
    "<a href=\"https://www.researchgate.net/figure/Proposed-Modified-ResNet-18-architecture-for-Bangla-HCR-In-the-diagram-conv-stands-for_fig1_323063171\"><img src=\"https://www.researchgate.net/profile/Muhammad_Hasan19/publication/323063171/figure/fig1/AS:603178554904576@1520820382219/Proposed-Modified-ResNet-18-architecture-for-Bangla-HCR-In-the-diagram-conv-stands-for.png\" alt=\"Proposed Modified ResNet-18 architecture for Bangla HCR. In the diagram, conv stands for Convolutional layer, Pool stands for MaxPool layer, batch norm stand for batch normalization, Relu stands for rectified linear unit activation layer, Sum stands for the addition in ResNet, and FC stand for fully connected hidden layers. In this architecture, we have eight ResNet modules which are modified by adding a dropout layer after the second convolutional layers.\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
    "\n",
    "class MNISTResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super(MNISTResNet, self).__init__(BasicBlock, [2, 2, 2, 2], num_classes=10) # Based on ResNet18\n",
    "        # super(MNISTResNet, self).__init__(BasicBlock, [3, 4, 6, 3], num_classes=10) # Based on ResNet34\n",
    "        # super(MNISTResNet, self).__init__(Bottleneck, [3, 4, 6, 3], num_classes=10) # Based on ResNet50\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3,bias=False)\n",
    "\n",
    "model = MNISTResNet()\n",
    "model.load_state_dict(torch.load('ResNet', map_location=device))  #<--------------------change model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disply pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = Image.open(r\"D:\\user\\Documents\\N26091194_TENG\\Projects\\MNIST\\pics\\22.jpg\") #<-----change your pics path\n",
    "X = composed(image)\n",
    "X = X[None, :, :]\n",
    "show_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推測 0 的機率: 0.0012 %\n",
      "推測 1 的機率: 99.7 %\n",
      "推測 2 的機率: 0.00162 %\n",
      "推測 3 的機率: 0.00255 %\n",
      "推測 4 的機率: 0.000786 %\n",
      "推測 5 的機率: 0.00102 %\n",
      "推測 6 的機率: 0.00671 %\n",
      "推測 7 的機率: 0.00539 %\n",
      "推測 8 的機率: 0.275 %\n",
      "推測 9 的機率: 0.00496 %\n",
      "----------------------------------------\n",
      "預測結果 1st : 1\n",
      "預測結果 2nd : 8\n",
      "預測結果 3rd : 6\n",
      "----------------------------------------\n",
      "預測為 : 1\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "z = model(X)\n",
    "z = nn.functional.softmax(z, dim=1)\n",
    "output = z.tolist()[0]\n",
    "output_copy = output.copy()\n",
    "\n",
    "for index, value in enumerate(output):\n",
    "    print('推測 '+str(index)+' 的機率: {:.3} %'.format(value * 100))\n",
    "print('-'*40)\n",
    "\n",
    "print('預測結果 1st :', output.index(max(output_copy)))\n",
    "output_copy.remove(max(output_copy))\n",
    "\n",
    "print('預測結果 2nd :', output.index(max(output_copy)))\n",
    "output_copy.remove(max(output_copy))\n",
    "\n",
    "print('預測結果 3rd :', output.index(max(output_copy)))\n",
    "output_copy.remove(max(output_copy))\n",
    "\n",
    "print('-'*40)\n",
    "print('預測為 :',output.index(max(output)))\n",
    "# model: ResNet\n",
    "# error in black : 0, 3, 4, 5, 6, 7, 9 (acc: 30%)\n",
    "# error in white : 6 (acc: 90%)\n",
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
